# SQUAD Question Answering using BERT

This repository demonstrates the use of transfer learning for SQUAD Question Answering using BERT from hugging face and was developed as a part of NLP coursework.

[![colab link](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/jaygala24/question-answering-bert/blob/master/question_answering.ipynb)

Goal: Develop a language model that can generate responses for questions (relevant to context) given a paragraph as context.

Abstract: In the task of reading comprehension or question answering, a model will be given a paragraph, and a question about that paragraph, as input. The goal is to answer the question correctly. From a research perspective, this is an interesting task because it provides a measure for how well systems can ‘understand’ text. From a more practical perspective, these systems have been extremely useful for better understanding any piece of text, and serving information needed by humans. The BERT model has revolutionized the field of Natural Language Processing. BERT based models are known to be proficient in handling context based dependencies.

Dataset used - SQUAD Question Answering Dataset
